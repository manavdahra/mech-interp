{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "Understand how the small transformer models come up with the algorithm of permuting elements in a list. \n",
    "What goes on in the QK and OV circuits. Are there more ways to explore what goes on in under the hood.\n",
    "\n",
    "The problem is borrowed from this post and it explains the motivation for solving it.\n",
    "https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability\n",
    "\n",
    "### Assumptions and constraints:\n",
    "1. A small transformer model (1 to 2-Layers) will be sufficient at coming up with correct permuations. To test this assumption. \n",
    "2. ReLU activation are sufficient to begin with. To test this assumption.\n",
    "3. Permutations of single groups shall be used for this exercise. **Explain what is meant by single groups here.**\n",
    "4. Context window will be set to 1024. Which means we may use at max 5-6 elements at most.\n",
    "5. Since the input data size is limited (max 5 elements), we may limit the vocab size to 62 characters (52 alphabets + 10 digits). Then we have, $62 \\choose 5$ different sequences to make sure model learns to generalize attention pattern on positions instead of characters seen from previous sequence term.\n",
    "6. A very simple custom tokenizer works fine for the provided dataset. To test this assumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (6.29.5)\n",
      "Requirement already satisfied: setuptools in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (73.0.1)\n",
      "Requirement already satisfied: transformer_lens in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: plotly in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (5.23.0)\n",
      "Requirement already satisfied: nbformat in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (5.10.4)\n",
      "Requirement already satisfied: circuitsvis in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (1.43.2)\n",
      "Requirement already satisfied: appnope in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (1.8.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (8.26.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.33.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (2.21.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.8.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.2.33)\n",
      "Requirement already satisfied: numpy>=1.26 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (2.2.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (13.7.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (2.4.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (4.66.5)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (4.44.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.17.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from nbformat) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: importlib-metadata>=5.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from circuitsvis) (8.4.0)\n",
      "Requirement already satisfied: pyyaml in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.24.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.4)\n",
      "Requirement already satisfied: filelock in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.10.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.20.0)\n",
      "Requirement already satisfied: decorator in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: sympy in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.27.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.13.0)\n",
      "Requirement already satisfied: setproctitle in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipykernel setuptools transformer_lens ipywidgets plotly nbformat circuitsvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722\n"
     ]
    }
   ],
   "source": [
    "n_ctx = len(\"> equ78 equ87 eq7u8 eq78u eq8u7 eq87u euq78 euq87 eu7q8 eu78q eu8q7 eu87q e7qu8 e7q8u e7uq8 e7u8q e78qu e78uq e8qu7 e8q7u e8uq7 e8u7q e87qu e87uq qeu78 qeu87 qe7u8 qe78u qe8u7 qe87u que78 que87 qu7e8 qu78e qu8e7 qu87e q7eu8 q7e8u q7ue8 q7u8e q78eu q78ue q8eu7 q8e7u q8ue7 q8u7e q87eu q87ue ueq78 ueq87 ue7q8 ue78q ue8q7 ue87q uqe78 uqe87 uq7e8 uq78e uq8e7 uq87e u7eq8 u7e8q u7qe8 u7q8e u78eq u78qe u8eq7 u8e7q u8qe7 u8q7e u87eq u87qe 7equ8 7eq8u 7euq8 7eu8q 7e8qu 7e8uq 7qeu8 7qe8u 7que8 7qu8e 7q8eu 7q8ue 7ueq8 7ue8q 7uqe8 7uq8e 7u8eq 7u8qe 78equ 78euq 78qeu 78que 78ueq 78uqe 8equ7 8eq7u 8euq7 8eu7q 8e7qu 8e7uq 8qeu7 8qe7u 8que7 8qu7e 8q7eu 8q7ue 8ueq7 8ue7q 8uqe7 8uq7e 8u7eq 8u7qe 87equ 87euq 87qeu 87que 87ueq 87uqe.\")\n",
    "print(n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from transformer_lens import EasyTransformerConfig, EasyTransformer\n",
    "import string\n",
    "\n",
    "special_chars = \"> .\"\n",
    "[beg, space, period] = list(special_chars)\n",
    "elements = special_chars + string.ascii_letters + string.digits\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "torch.set_default_device(\"mps\")\n",
    "\n",
    "cfg = EasyTransformerConfig(\n",
    "    n_layers=1,\n",
    "    d_model=128,\n",
    "    d_head=64,\n",
    "    n_heads=1,\n",
    "    # d_mlp=64,\n",
    "    d_mlp=None,\n",
    "    d_vocab=len(elements),\n",
    "    n_ctx=122, #> abcd abdc acbd acdb adbc adcb bacd badc bcad bcda bdac bdca cabd cadb cbad cbda cdab cdba dabc dacb dbac dbca dcab dcba.\n",
    "    act_fn=\"solu\",\n",
    "    # act_fn=\"relu\",\n",
    "    attn_only=True,\n",
    ")\n",
    "model = EasyTransformer(cfg)\n",
    "\n",
    "def str_to_tokens(s: str) -> List[int]:\n",
    "    return [elements.index(ch) for ch in s]\n",
    "\n",
    "def tokens_to_str(tokens: List[int]) -> str:\n",
    "    return \"\".join([elements[token] for token in tokens])\n",
    "\n",
    "print(model)\n",
    "print(model.cfg.d_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data generator\n",
    "1. Use `itertools` library to first pick a combination of elements from vocabulary of 62 charcaters. See [itertools](https://docs.python.org/3/library/itertools.html#itertools.combinations)\n",
    "2. Generate permutation sequence from given elements. See [itertools](https://docs.python.org/3/library/itertools.html#itertools.permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> GZ5 G5Z ZG5 Z5G 5GZ 5ZG.\n",
      "> F045 F054 F405 F450 F504 F540 0F45 0F54 04F5 045F 05F4 054F 4F05 4F50 40F5 405F 45F0 450F 5F04 5F40 50F4 504F 54F0 540F.\n",
      "> equ78 equ87 eq7u8 eq78u eq8u7 eq87u euq78 euq87 eu7q8 eu78q eu8q7 eu87q e7qu8 e7q8u e7uq8 e7u8q e78qu e78uq e8qu7 e8q7u e8uq7 e8u7q e87qu e87uq qeu78 qeu87 qe7u8 qe78u qe8u7 qe87u que78 que87 qu7e8 qu78e qu8e7 qu87e q7eu8 q7e8u q7ue8 q7u8e q78eu q78ue q8eu7 q8e7u q8ue7 q8u7e q87eu q87ue ueq78 ueq87 ue7q8 ue78q ue8q7 ue87q uqe78 uqe87 uq7e8 uq78e uq8e7 uq87e u7eq8 u7e8q u7qe8 u7q8e u78eq u78qe u8eq7 u8e7q u8qe7 u8q7e u87eq u87qe 7equ8 7eq8u 7euq8 7eu8q 7e8qu 7e8uq 7qeu8 7qe8u 7que8 7qu8e 7q8eu 7q8ue 7ueq8 7ue8q 7uqe8 7uq8e 7u8eq 7u8qe 78equ 78euq 78qeu 78que 78ueq 78uqe 8equ7 8eq7u 8euq7 8eu7q 8e7qu 8e7uq 8qeu7 8qe7u 8que7 8qu7e 8q7eu 8q7ue 8ueq7 8ue7q 8uqe7 8uq7e 8u7eq 8u7qe 87equ 87euq 87qeu 87que 87ueq 87uqe.\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations, combinations\n",
    "from torch.utils.data import Dataset\n",
    "from random import shuffle\n",
    "\n",
    "def permute(elements: str) -> List[str]:\n",
    "    return [\"\".join(item) for item in permutations(elements, len(elements))]\n",
    "\n",
    "def data_from_permute(terms: List[str]) -> str:\n",
    "    output = space.join(terms)\n",
    "    \n",
    "    return f\"> {output}.\"\n",
    "\n",
    "def generate_dataset(n, combs=None):\n",
    "    if not combs:\n",
    "        elems = elements[len(special_chars):]\n",
    "        combs = list(combinations(elems, n))\n",
    "        shuffle(combs)\n",
    "    # excluding special tokens in the beginning\n",
    "    while True:\n",
    "        for comb in combs:\n",
    "            terms = permute(\"\".join(comb))\n",
    "            yield data_from_permute(terms)\n",
    "\n",
    "class PermutationDataset(Dataset):\n",
    "    def __init__(self, n=5, max_len=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.generator = generate_dataset(n)\n",
    "        self.n = n\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.max_len\n",
    "        \n",
    "    def __getitem__(self, _) -> dict:\n",
    "        while True:\n",
    "            for entry in self.generator:\n",
    "                tokens = torch.tensor(str_to_tokens(entry), device=\"mps\")\n",
    "                return {\"tokens\": tokens}\n",
    "\n",
    "print(next(generate_dataset(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "1. Data set generator should generate expected data.\n",
    "2. Conversion of `str_to_tokens` and `tokens_to_string` should do conversions correctly.\n",
    "2. Model should work correctly for a sample data. Tensor shapes should match with expected shape.\n",
    "3. Model should incorrectly predict the permuation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     actual_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(test_dataset_gen)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m expected_data \u001b[38;5;241m==\u001b[39m actual_data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mshould_generate_correct_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m> a.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m should_generate_correct_dataset(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> ab ba.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m should_generate_correct_dataset(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> abc acb bac bca cab cba.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mshould_generate_correct_dataset\u001b[0;34m(n, expected_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshould_generate_correct_dataset\u001b[39m(n: \u001b[38;5;28mint\u001b[39m, expected_data: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     test_dataset_gen \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset\u001b[49m(n, combs\u001b[38;5;241m=\u001b[39mcombinations(elements[\u001b[38;5;28mlen\u001b[39m(special_chars):], n))\n\u001b[1;32m      3\u001b[0m     actual_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(test_dataset_gen)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m expected_data \u001b[38;5;241m==\u001b[39m actual_data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def should_generate_correct_dataset(n: int, expected_data: str):\n",
    "    test_dataset_gen = generate_dataset(n, combs=combinations(elements[len(special_chars):], n))\n",
    "    actual_data = next(test_dataset_gen)\n",
    "    assert expected_data == actual_data, f\"{expected_data} != {actual_data}\"\n",
    "\n",
    "should_generate_correct_dataset(1, \"> a.\")\n",
    "should_generate_correct_dataset(2, \"> ab ba.\")\n",
    "should_generate_correct_dataset(3, \"> abc acb bac bca cab cba.\")\n",
    "should_generate_correct_dataset(4, \"> abcd abdc acbd acdb adbc adcb bacd badc bcad bcda bdac bdca cabd cadb cbad cbda cdab cdba dabc dacb dbac dbca dcab dcba.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_return_output_tensor_with_correct_shape(input: torch.Tensor, expected_shape: List[int]):\n",
    "    expected_shape = torch.Size(expected_shape)\n",
    "    output = model(input)\n",
    "    actual_shape = output.shape\n",
    "    \n",
    "    assert expected_shape == actual_shape, f\"{expected_shape} != {actual_shape}\"\n",
    "    \n",
    "def should_return_output_tensor_with_incorrect_prediction(input: str, expected_completion: str):\n",
    "    tokens = torch.tensor(str_to_tokens(input))\n",
    "    output = model(tokens)\n",
    "    \n",
    "    actual_completion = tokens_to_str([output[:, -1, :].argmax().item()])\n",
    "    assert expected_completion != actual_completion, f\"{expected_completion} != {actual_completion}\"\n",
    "\n",
    "test_dataset_gen = generate_dataset(2)\n",
    "test_input_seq = next(test_dataset_gen)\n",
    "test_tokens = str_to_tokens(test_input_seq)\n",
    "input = torch.tensor(test_tokens)\n",
    "\n",
    "should_return_output_tensor_with_correct_shape(input, [1, len(test_tokens), model.cfg.d_vocab])\n",
    "should_return_output_tensor_with_incorrect_prediction(\"> ab ab.\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample run for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_test(sample: str):\n",
    "    tokens = str_to_tokens(sample)\n",
    "    i = 1\n",
    "    output = \"\"\n",
    "    while i < len(tokens):\n",
    "        input = tokens[:i]\n",
    "        logits = model(torch.tensor(input))\n",
    "        prediction = logits[:, -1, :].argmax().item()\n",
    "        # prediction = tokens_to_str(prediction)\n",
    "        output += tokens_to_str([prediction])\n",
    "        i += 1\n",
    "    print(f\"input:{sample}\")\n",
    "    print(f\"prediction:{output}\")\n",
    "\n",
    "sample_data_test(next(generate_dataset(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "1. Use synthetic data to train the model.\n",
    "2. Observe and analyse the loss. Generate graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_fn(logits, tokens, return_per_token=False):\n",
    "    # logits: [batch, position, d_vocab]\n",
    "    # tokens: [batch, position]\n",
    "    logits = logits[:, :-1]\n",
    "    tokens = tokens[:, 1:]\n",
    "    \n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]\n",
    "    if return_per_token:\n",
    "        return -correct_log_probs\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "def collate_fn(data):\n",
    "    max_size = max(d[\"tokens\"].size(0) for d in data)\n",
    "    padded_data = torch.stack([F.pad(d[\"tokens\"], (0, max_size-d[\"tokens\"].size(0)), value=0) for d in data])\n",
    "    \n",
    "    return padded_data\n",
    "\n",
    "dataset=PermutationDataset(n=4, max_len=122)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    generator=torch.Generator(device=\"mps\"), \n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data loader shape and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from transformer_lens import EasyTransformer\n",
    "from torch import optim\n",
    "from transformer_lens.train import train\n",
    "import os\n",
    "\n",
    "def train(\n",
    "    model: EasyTransformer,\n",
    "    num_epochs=100,\n",
    "    lr=1e-3,\n",
    "    max_grad_norm=1.0,\n",
    "    print_every=100,\n",
    "    save_dir=\"./model_weights\",\n",
    "    betas=(.9, .99),\n",
    "    save_every=None,\n",
    "    max_steps = None,\n",
    ") -> List[float]:\n",
    "    model.zero_grad()\n",
    "    model.init_weights()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, betas=betas)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=100)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in trange(1, num_epochs + 1):\n",
    "        samples = 0\n",
    "        for step, tokens in enumerate(dataloader):\n",
    "            logits = model(tokens)\n",
    "            loss = loss_fn(logits, tokens)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "            samples += tokens.shape[0]\n",
    "\n",
    "            if save_every is not None and step % save_every == 0 and save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{save_dir}/model_{step}.pt\")\n",
    "                \n",
    "            if print_every is not None and step % print_every == 0:\n",
    "                print(f\"Epoch {epoch} Samples {samples} Step {step} Loss {loss.item()}\")\n",
    "\n",
    "            if max_steps is not None and step >= max_steps:\n",
    "                break\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses = train(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(losses, labels={\"index\": \"steps\", \"value\": \"loss\", \"title\": \"Loss vs steps\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why is model training showing spike ?\n",
    "\n",
    "> Why is model loss not converging to 0 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit analysis and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.attention import attention_heads\n",
    "\n",
    "def run_with_cache(prompt):\n",
    "    cache = None\n",
    "    input_tokens = str_to_tokens(prompt)\n",
    "    while len(input_tokens) < cfg.n_ctx:\n",
    "        output, cache = model.run_with_cache(torch.tensor(input_tokens))\n",
    "        predicted = output[:, -1, :].argmax().item()\n",
    "        input_tokens.append(predicted)\n",
    "    \n",
    "    return tokens_to_str(input_tokens), cache\n",
    "\n",
    "def analyze_attention_heads():\n",
    "    expected = next(generate_dataset(4))\n",
    "    prompt = \" \".join(expected.split(\" \")[:2])\n",
    "    \n",
    "    print(f\"prompt:{prompt}\")\n",
    "    completion, cache = run_with_cache(prompt)\n",
    "    \n",
    "    print(f\"expected:{expected}\")\n",
    "    print(f\"completion:{completion}\")\n",
    "    \n",
    "    return expected, completion, cache\n",
    "\n",
    "expected, completion, cache = analyze_attention_heads()\n",
    "attention_heads(tokens=list(completion), attention=cache[\"pattern\", 0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.tokens import colored_tokens_multi\n",
    "\n",
    "print(completion)\n",
    "colored_tokens_multi(tokens=list(completion[:-1]), values=cache[\"pattern\", 0][-1, -1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

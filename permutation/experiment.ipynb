{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Can a 1-Layer transformer model with only a 1 attention head learn to predict correct premutation seguences ?\n",
    "\n",
    "1. Can we possibly understand how the models develops an algorithm of permuting elements in a list ?\n",
    "2. What goes on in the QK circuits ? (OV circuits are not in guestion as we there are no MLP layers)\n",
    "\n",
    "This experiment is an attempt to answer the above guestions.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "This experiment is divised to solve one of concrete open problems proposed by Neel Nanda here:\n",
    "\n",
    "https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability\n",
    "\n",
    "### Assumptions and constraints:\n",
    "\n",
    "Like any decent experiment, we first need to come up with assumptions and constraints that limit the scope of the problem so some progress can be tracked.\n",
    "\n",
    "Following are some assumptions & constraints for this experiment:\n",
    "1. A 1-Layer attention only transformer model is sufficient to do correct permuations term predictions.\n",
    "2. ReLU activations are sufficient to begin with. Incorrect, SoLU did better.\n",
    "3. Permutations of full complete groups shall be used for this exercise. Full complete groups implies - all elements are to be used to generate permutation.\n",
    "4. Context window will be set to a fixed length and fixed permuation size. Analysis will be done for max permutation size of 5 elements.\n",
    "5. The limit of vocab size is 62 characters (52 alphabets + 10 digits). There is a possibility of $62 \\choose 5$ different sequences to make sure model learns to generalize attention pattern on positions instead of characters seen from previous sequence term.\n",
    "6. Custom tokenizer (similar to ascii-coding) is used for the provided dataset. This makes the experiment setup and we can only focus on attention mechanisms of transformer which is the meat of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipykernel setuptools transformer_lens ipywidgets plotly nbformat circuitsvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Vocabulary and Synthetic data generation:\n",
    "\n",
    "Example generation:\n",
    "\n",
    "> \\> equ78 equ87 eq7u8 eq78u eq8u7 eq87u euq78 euq87 eu7q8 eu78q eu8q7 eu87q e7qu8 e7q8u e7uq8 e7u8q e78qu e78uq e8qu7 e8q7u e8uq7 e8u7q e87qu e87uq qeu78 qeu87 qe7u8 qe78u qe8u7 qe87u que78 que87 qu7e8 qu78e qu8e7 qu87e q7eu8 q7e8u q7ue8 q7u8e q78eu q78ue q8eu7 q8e7u q8ue7 q8u7e q87eu q87ue ueq78 ueq87 ue7q8 ue78q ue8q7 ue87q uqe78 uqe87 uq7e8 uq78e uq8e7 uq87e u7eq8 u7e8q u7qe8 u7q8e u78eq u78qe u8eq7 u8e7q u8qe7 u8q7e u87eq u87qe 7equ8 7eq8u 7euq8 7eu8q 7e8qu 7e8uq 7qeu8 7qe8u 7que8 7qu8e 7q8eu 7q8ue 7ueq8 7ue8q 7uqe8 7uq8e 7u8eq 7u8qe 78equ 78euq 78qeu 78que 78ueq 78uqe 8equ7 8eq7u 8euq7 8eu7q 8e7qu 8e7uq 8qeu7 8qe7u 8que7 8qu7e 8q7eu 8q7ue 8ueq7 8ue7q 8uqe7 8uq7e 8u7eq 8u7qe 87equ 87euq 87qeu 87que 87ueq 87uqe.\n",
    "\n",
    "$n_{ctx} = 722$\n",
    "\n",
    "where $n_{ctx}$ is the context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from typing import List\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self._special_chars = \"> .\"\n",
    "        self.vocab = self._special_chars + string.ascii_letters + string.digits\n",
    "\n",
    "    def str_to_tokens(self, s: str) -> List[int]:\n",
    "        return [self.vocab.index(ch) for ch in s]\n",
    "\n",
    "    def tokens_to_str(self, tokens: List[int]) -> str:\n",
    "        return \"\".join([self.vocab[token] for token in tokens])\n",
    "    \n",
    "    def special_chars(self) -> List[str]:\n",
    "        return list(self._special_chars)\n",
    "        \n",
    "    def vocab_without_special_chars(self) -> str:\n",
    "        return self.vocab[len(self._special_chars):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data\n",
    "\n",
    "1. Use `itertools` library to generate a combination of elements from vocabulary of 62 charcaters. See [itertools](https://docs.python.org/3/library/itertools.html#itertools.combinations)\n",
    "2. Generate permutation sequence from given elements. See [itertools](https://docs.python.org/3/library/itertools.html#itertools.permutations)\n",
    "\n",
    "Properties of data:\n",
    "Sample prompt:\n",
    "> abcd abdc acbd acdb adbc adcb bacd badc bcad bcda bdac bdca cabd cadb cbad cbda cdab cdba dabc dacb dbac dbca dcab dcba.\n",
    "\n",
    "\n",
    "Total characters:\n",
    "\n",
    "$$seq\\_len(n) = \\underbrace{(n!)}_{\\text{sequence terms}} \\times \\underbrace{(n+1)}_{\\text{term length + space}} + \\underbrace{2}_{\\text{special chars}}$$\n",
    "$$seq\\_len(n) = (n+1)! + 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import shuffle\n",
    "from itertools import permutations, combinations\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "n_comb = 4\n",
    "tokenizer = Tokenizer()\n",
    "[beg, space, period] = tokenizer.special_chars()\n",
    "\n",
    "def sequence_len(n: int) -> int:\n",
    "    return math.factorial(n+1)+2\n",
    "\n",
    "def start_pos(seq_len: int) -> int: # reverse mapping for seq_len vs offset\n",
    "    return {8: 4, 26: 5, 122: 6, 722: 7}[seq_len]\n",
    "\n",
    "def permute(vocab: str) -> List[str]:\n",
    "    return [\"\".join(item) for item in permutations(vocab, len(vocab))]\n",
    "\n",
    "def data_from_permute(terms: List[str]) -> str:\n",
    "    output = space.join(terms)\n",
    "    \n",
    "    return f\"> {output}.\"\n",
    "\n",
    "def generate_dataset(n, combs=None):\n",
    "    if not combs:\n",
    "        elems = tokenizer.vocab_without_special_chars()\n",
    "        combs = list(combinations(elems, n))\n",
    "        shuffle(combs)\n",
    "    \n",
    "    while True:\n",
    "        for comb in combs:\n",
    "            terms = permute(\"\".join(comb))\n",
    "            yield data_from_permute(terms)\n",
    "\n",
    "n_ctx = sequence_len(n_comb)\n",
    "print(n_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test generator function and tokenizer\n",
    "1. Data set generator should generate expected data.\n",
    "2. Conversion of `str_to_tokens` and `tokens_to_string` should do conversions correctly.\n",
    "2. Model should work correctly for a sample data. Tensor shapes should match with expected shape.\n",
    "3. Model should incorrectly predict the permuation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_generate_correct_dataset(n: int, expected_data: str):\n",
    "    test_dataset_gen = generate_dataset(n, combs=combinations(tokenizer.vocab_without_special_chars(), n))\n",
    "    actual_data = next(test_dataset_gen)\n",
    "    assert expected_data == actual_data, f\"{expected_data} != {actual_data}\"\n",
    "\n",
    "should_generate_correct_dataset(1, \"> a.\")\n",
    "should_generate_correct_dataset(2, \"> ab ba.\")\n",
    "should_generate_correct_dataset(3, \"> abc acb bac bca cab cba.\")\n",
    "should_generate_correct_dataset(4, \"> abcd abdc acbd acdb adbc adcb bacd badc bcad bcda bdac bdca cabd cadb cbad cbda cdab cdba dabc dacb dbac dbca dcab dcba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "\n",
    "1. n_layers = 1\n",
    "2. d_model=128\n",
    "3. d_head=64\n",
    "4. n_heads=1\n",
    "5. d_vocab=65\n",
    "6. attn_only=True\n",
    "7. n_ctx=len(prompt) (722 max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import EasyTransformerConfig, EasyTransformer\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "torch.set_default_device(\"mps\")\n",
    "\n",
    "cfg = EasyTransformerConfig(\n",
    "    n_layers=1,\n",
    "    d_model=128,\n",
    "    d_head=64,\n",
    "    n_heads=1,\n",
    "    d_mlp=None,\n",
    "    d_vocab=len(tokenizer.vocab),\n",
    "    n_ctx=n_ctx,\n",
    "    act_fn=\"solu\", # think about what activation function is best. SoLU does better than ReLU.\n",
    "    attn_only=True,\n",
    ")\n",
    "model = EasyTransformer(cfg)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_return_output_tensor_with_correct_shape(input: torch.Tensor, expected_shape: List[int]):\n",
    "    with torch.no_grad():\n",
    "        expected_shape = torch.Size(expected_shape)\n",
    "        output = model(input)\n",
    "        actual_shape = output.shape\n",
    "    \n",
    "    assert expected_shape == actual_shape, f\"{expected_shape} != {actual_shape}\"\n",
    "    \n",
    "def should_return_output_tensor_with_incorrect_prediction(input: str, expected_completion: str):\n",
    "    with torch.no_grad():\n",
    "        tokens = torch.tensor(tokenizer.str_to_tokens(input))\n",
    "        output = model(tokens)\n",
    "    \n",
    "    actual_completion = tokenizer.tokens_to_str([output[:, -1, :].argmax().item()])\n",
    "    assert expected_completion != actual_completion, f\"{expected_completion} != {actual_completion}\"\n",
    "\n",
    "test_dataset_gen = generate_dataset(2)\n",
    "test_input_seq = next(test_dataset_gen)\n",
    "test_input = tokenizer.str_to_tokens(test_input_seq)\n",
    "test_input = torch.tensor(test_input)\n",
    "\n",
    "should_return_output_tensor_with_correct_shape(test_input, [1, len(test_input), model.cfg.d_vocab])\n",
    "should_return_output_tensor_with_incorrect_prediction(\"> ab\", \"> ab ba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model prediction on sample data set.\n",
    "\n",
    "Test model outputs before it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_test(expected: str):\n",
    "    with torch.no_grad():\n",
    "        offset = start_pos(len(expected))\n",
    "        tokens = tokenizer.str_to_tokens(expected)\n",
    "        prefix = tokens[:offset]\n",
    "        tokens = torch.tensor(tokens)\n",
    "        i = offset\n",
    "        output = []\n",
    "        while i < len(expected):\n",
    "            logits = model(tokens[:i])\n",
    "            prediction = logits[:, -1, :].argmax().item()\n",
    "            output.append(prediction)\n",
    "            i += 1\n",
    "        print(f\"expected:{expected}\")\n",
    "        print(f\"actual:{tokenizer.tokens_to_str(prefix + output)}\")\n",
    "\n",
    "sample_data_test(next(generate_dataset(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps, not surprisingly, the model outputs gibberish. It cannot do sequence prediction yet, since it is not trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set with generator functions\n",
    "1. Generator functions are handy tools for generating data lazily. Here `__getitem__` is a generator function\n",
    "2. This becomes very handy when we deal with lots of data and can't load it to memory to work from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from random import choice\n",
    "\n",
    "class PermutationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n:int=n_comb, \n",
    "        max_len:int=n_ctx, \n",
    "        all_combs:bool=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        generators = [generate_dataset(n)]\n",
    "        if all_combs:\n",
    "            generators = [generate_dataset(i) for i in range(2, n+1)]\n",
    "        \n",
    "        self.generators = generators\n",
    "        self.n = n\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return 1000\n",
    "        \n",
    "    def __getitem__(self, _) -> dict:\n",
    "        while True:\n",
    "            for entry in choice(self.generators):\n",
    "                tokens = torch.tensor(tokenizer.str_to_tokens(entry), device=\"mps\")\n",
    "                return {\"tokens\": tokens, \"length\": len(entry)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loader\n",
    "\n",
    "1. $test \\ratio train = 20 \\ratio 80$\n",
    "2. `collate_fn` is defined so we can train the model on variable sequence length (if needed).\n",
    "3. For now, only fixed length dataset is generated. This is to make the attention head pattern more comprehensible. \n",
    "\n",
    "> To study the pattern for variable sequence length, just feed `all_combs=True` to `PermutationDataset` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(data):\n",
    "    pad_token = tokenizer.special_chars().index(space)\n",
    "    batch_size = len(data)\n",
    "    max_len = max([d[\"length\"] for d in data])\n",
    "    \n",
    "    padded_data = torch.stack([F.pad(d[\"tokens\"], (0, max_len-d[\"length\"]), value=pad_token) for d in data])\n",
    "    attention_mask = torch.tensor([start_pos(d[\"length\"])-1 for d in data]).reshape(batch_size, 1)\n",
    "    attention_mask = (torch.arange(max_len).repeat(batch_size).reshape(batch_size, max_len) >= attention_mask).int()\n",
    "    \n",
    "    return {\"tokens\": padded_data, \"attention_mask\": attention_mask}\n",
    "\n",
    "# Switching of all_combs=True flag for now. To do an easier the analysis.\n",
    "dataset = PermutationDataset(n=n_comb, max_len=n_ctx)\n",
    "generator = torch.Generator(device=\"mps\")\n",
    "test, train = random_split(dataset=dataset, lengths=[.2, .8], generator=generator)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train, \n",
    "    batch_size=4, \n",
    "    generator=generator,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test, \n",
    "    batch_size=4, \n",
    "    generator=generator, \n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test shape and content of Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_batch_size = 4\n",
    "\n",
    "for index, batch in enumerate(train_data_loader):\n",
    "    for tokens, attn_mask in zip(batch[\"tokens\"], batch[\"attention_mask\"]):\n",
    "        print(f\"{tokenizer.tokens_to_str(tokens)}\")\n",
    "    \n",
    "    assert expected_batch_size == batch[\"tokens\"].shape[0], f\"{expected_batch_size} != {batch.shape[0]}\"\n",
    "    if index == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "1. Use synthetically generated training data from dataloader to train the model.\n",
    "2. Collect loss data at every step and generate loss vs steps graph to validate that the loss is decreasing.\n",
    "3. Loss function computes the `log_softmax` of the logits for every token predicted in the sequence.\n",
    "4. Attention mask is used to ignore tokens until first sequence term for loss calulcation. Sequence cannot be predicted until first sequence term is supplied.\n",
    "\n",
    "### Hyper parameters:\n",
    "1. learning rate    =0.0001\n",
    "2. num of epochs    =10 (default)\n",
    "3. optimizer betas  =(0.9, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from transformer_lens import EasyTransformer\n",
    "from transformer_lens.utils import lm_cross_entropy_loss\n",
    "from torch import optim\n",
    "from transformer_lens.train import train\n",
    "import os\n",
    "\n",
    "def train(\n",
    "    model: EasyTransformer,\n",
    "    num_epochs=10,\n",
    "    lr=1e-4,\n",
    "    max_grad_norm=1.0,\n",
    "    print_every=100,\n",
    "    save_dir=\"./model_weights\",\n",
    "    betas=(.9, .99),\n",
    "    save_every=None,\n",
    "    max_steps = None,\n",
    ") -> List[float]:\n",
    "    model.zero_grad()\n",
    "    model.init_weights()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, betas=betas)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in trange(1, num_epochs + 1):\n",
    "        samples = 0\n",
    "        for step, batch in enumerate(train_data_loader):\n",
    "            tokens = batch[\"tokens\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            logits = model(input=tokens)\n",
    "            \n",
    "            loss = lm_cross_entropy_loss(logits, tokens, attention_mask)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            samples += tokens.shape[0]\n",
    "\n",
    "            if save_every is not None and step % save_every == 0 and save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{save_dir}/model_{step}.pt\")\n",
    "                \n",
    "            if print_every is not None and step % print_every == 0:\n",
    "                print(f\"Epoch {epoch} Samples {samples} Step {step} Loss {loss.item()}\")\n",
    "\n",
    "            if max_steps is not None and step >= max_steps:\n",
    "                break\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses = train(model=model, num_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss vs steps graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(losses, labels={\"index\": \"steps\", \"value\": \"loss\", \"title\": \"Loss vs steps\", \"variable\": \"loss\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"loss_vs_steps_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "This is promising!\n",
    "\n",
    "Clearly the loss decreases rapidly within first few epochs. \n",
    "\n",
    "Hence, model must have learned to do permutation sequence prediction given what we have observed.\n",
    "\n",
    "We can validate this observation by testing on a test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model accuracy\n",
    "1. Accuracy function is defined similar to a loss function before, except this time we count how many tokens in the predicted sequence match the expected sequence tokens.\n",
    "2. Accuracy computation is done ignoring the first `2+n_comb` positions. This is needed to make sure model is not penalised unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import lm_accuracy\n",
    "\n",
    "def validate_model(model: EasyTransformer):\n",
    "    with torch.no_grad():\n",
    "        accuracies = []\n",
    "        for _, batch in enumerate(test_data_loader):\n",
    "            logits = model(input=batch[\"tokens\"])\n",
    "            accuracy = lm_accuracy(logits, batch[\"tokens\"])\n",
    "            accuracies.append(accuracy)\n",
    "    \n",
    "    return sum(accuracies)/len(accuracies)\n",
    "\n",
    "accuracy = validate_model(model)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is very close to `~1.0`!\n",
    "\n",
    "We may conclude that the model has indeed learned how to generate a permutation sequence. Where the length of the sequence is between the range of [2, 5].\n",
    "\n",
    "Let's check if the output being generated by model is accurate enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(batch: torch.Tensor):\n",
    "    attn_mask = batch[\"attention_mask\"]\n",
    "    tokens = batch[\"tokens\"]\n",
    "    logits = model(input=tokens)\n",
    "    \n",
    "    predictions = logits.argmax(-1)\n",
    "    for i in range(len(predictions)):\n",
    "        print(tokenizer.tokens_to_str(tokens[i]))\n",
    "        zeros = attn_mask.shape[1] - torch.count_nonzero(attn_mask[i]).item()\n",
    "        prefix = torch.cat((tokens[i,:zeros+1], predictions[i,zeros:]), dim=-1)\n",
    "        print(tokenizer.tokens_to_str(prefix))\n",
    "        \n",
    "    print(f\"Accuracy: {lm_accuracy(logits, batch[\"tokens\"])}\")\n",
    "    print(f\"Loss: {lm_cross_entropy_loss(logits, batch[\"tokens\"], attn_mask)}\")\n",
    "\n",
    "batch = next(iter(train_data_loader))\n",
    "check(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty accurate! We can now try to understand what model has learned. Maybe we can get some insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit analysis and observations\n",
    "\n",
    "Since we have only 1 attention and 1 layer, this means all the permutation generation logic is encoded in the single attention head.\n",
    "\n",
    "Let's see if we can analyze what that attention head has actually learned after training.\n",
    "\n",
    "We can start by plotting a heat map of attention scores between every pair of positions $(i,j)$ \n",
    "\n",
    "where, $i = source$ and $j = destination$.\n",
    "\n",
    "**Moreover, we know by our design that there is no MLP layer, hence, the model is not really concerning itself with token values (no OV circuit), but only their relative positions of the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(input, tensor, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    plot_kwargs = {\n",
    "        \"color_continuous_scale\":\"RdBu\", \n",
    "        \"color_continuous_midpoint\":0.0, \n",
    "        \"labels\":{\"x\": xaxis, \"y\": yaxis}, \n",
    "        \"width\": 1024, \n",
    "        \"height\": 1024,\n",
    "        \"x\": [f\"{c},{i}\" for i, c in enumerate(input)],\n",
    "        \"y\": [f\"{c},{i}\" for i, c in enumerate(input)],\n",
    "    }\n",
    "    plot_kwargs.update(kwargs)\n",
    "    fig = px.imshow(tensor, **plot_kwargs)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import to_numpy\n",
    "\n",
    "test_input = next(generate_dataset(n=4))\n",
    "test_input_tokens = tokenizer.str_to_tokens(test_input)\n",
    "logits, cache = model.run_with_cache(torch.tensor(test_input_tokens))\n",
    "\n",
    "attn_pattern = to_numpy(cache[\"pattern\", 0])\n",
    "print(attn_pattern.shape)\n",
    "\n",
    "limit = sequence_len(n_comb)\n",
    "imshow(test_input[:limit], attn_pattern[-1,-1][:limit,:limit], xaxis=\"src_token\", yaxis=\"dst_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Given permutation sequence starting with `> MNR5`\n",
    "\n",
    "<img src=\"./attn_heads_heat_map_1.png\">\n",
    "\n",
    "1. Zooming in, it is clear that the permutation sequence is being generated by doing copy operations by the QK circuit. Note, there is no OV circuit in this model.\n",
    "    \n",
    "    <img src=\"./analysis_copy_head.png\" height=500>\n",
    "    \n",
    "    Here in this graph, we can see that attention copies the token from position 14 to position 19. In this case `N` is copied to the position where `5` is. \n",
    "    The attention head is 99% certain that copying the token from 14 to 19 position is the right thing to do. This learning happened during the training phase.\n",
    "    \n",
    "    Following this, the next `dst_token` at position 20 is `N` (as expected) and it has 2 activations both of which are `\" \"` coming from 2 different positions `16 & 1`. \n",
    "    Since, both are correct choices, model choses ` ` from `src_postion = 1` in this case. The process continues. \n",
    "2. Activations of `src_token = \" \"` takes up regular intervals of `5` in vertical direction.\n",
    "    \n",
    "    This makes sense, as the term length is of `5` characters and activation/copying of `\" \"` will happen regularly.\n",
    "3. More interesting question is what is the pattern of activations of non-space characters ? \n",
    "    \n",
    "    Other characters don't follow a strict spacing pattern of activation, as the characters once in a while do a swap in subsequent terms. \n",
    "    Due to this, the spacing interval is not regular and grows and shrinks regularly.\n",
    "    I have not been able to work out the mathematics for this yet. But I suspect it is something that other researchers might have already figured out.\n",
    "\n",
    "4. There are no significant activations/copying of `src_tokens` beyond position number `15`\n",
    "\n",
    "    One explanation could be that since the characters repeat quite a lot in the sequence, it is sufficient for the model to just pay attention to first few character positions to predict the whole sequence. Similar to the explanation of 2 activations of space at position number `20`.\n",
    "    We can test this assumption by shrinking the context length of the model under `122` characters, the model should then activate more positions in the lower diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.tokens import colored_tokens_multi\n",
    "\n",
    "colored_tokens_multi(tokens=list(test_input), values=cache[\"pattern\", 0][-1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. What happens if we feed duplicate characters, can it predict correctly ?\n",
    "2. Why top left corner of the attention head pattern shows higher scores (darker blue colors) ? \n",
    "3. What does the figure showing attention pattern mean ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can the model do sequence generation for duplicate tokens ?\n",
    "\n",
    "Yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character g is duplicate\n",
    "dup_data = \"ghg4 gh4g ggh4 gg4h g4hg g4gh hgg4 hg4g hgg4 hg4g h4gg h4gg ggh4 gg4h ghg4 gh4g g4gh g4hg 4ghg 4ggh 4hgg 4hgg 4ggh 4ghg.\"\n",
    "sample_data_test(dup_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do initial positions in the attention head pattern have higher scores.\n",
    "\n",
    "One explanation could be that in order for the transformer to predict the correct sequence, it needs 2 key information:\n",
    "1. First n-tokens in the sequence to identify initial relative ordering of tokens\n",
    "2. Current token  because we have large context length, that is max sequence size (722)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

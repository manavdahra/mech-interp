{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Can a 1-Layer transformer model with only a 1 attention head learn to predict correct premutation sequences ?\n",
    "\n",
    "1. Can we possibly understand how the models develops an algorithm of permuting elements in a list ?\n",
    "2. What goes on in the QK circuits ? (OV circuits are not in guestion as we there are no MLP layers)\n",
    "\n",
    "This experiment is an attempt to answer the above questions.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "This experiment is divised to solve one of concrete open problems proposed by Neel Nanda here:\n",
    "\n",
    "https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability\n",
    "\n",
    "### Assumptions and constraints:\n",
    "\n",
    "Like any decent experiment, we first need to come up with assumptions and constraints that limit the scope of the problem so some progress can be tracked.\n",
    "\n",
    "Following are some assumptions & constraints for this experiment:\n",
    "1. A 1-Layer attention only transformer model is sufficient to do correct permuations term predictions.\n",
    "2. ReLU activations are sufficient to begin with. Incorrect, SoLU did better.\n",
    "3. Permutations of full complete groups shall be used for this exercise. Full complete groups implies - all elements are to be used to generate permutation.\n",
    "4. Context window will be set to a fixed length and fixed permuation size. Analysis will be done for max permutation size of 5 elements.\n",
    "5. The limit of vocab size is 62 characters (52 alphabets + 10 digits). There is a possibility of $62 \\choose 5$ different sequences to make sure model learns to generalize attention pattern on positions instead of characters seen from previous sequence term.\n",
    "6. Custom tokenizer (similar to ascii-coding) is used for the provided dataset. This makes the experiment setup and we can only focus on attention mechanisms of transformer which is the meat of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (6.29.5)\n",
      "Requirement already satisfied: setuptools in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (73.0.1)\n",
      "Requirement already satisfied: transformer_lens in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: plotly in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (5.23.0)\n",
      "Requirement already satisfied: nbformat in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (5.10.4)\n",
      "Requirement already satisfied: circuitsvis in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (1.43.2)\n",
      "Requirement already satisfied: appnope in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (1.8.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (8.26.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.33.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (2.21.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.8.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.2.33)\n",
      "Requirement already satisfied: numpy>=1.26 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (2.2.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (13.7.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (2.4.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (4.66.5)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (4.44.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformer_lens) (0.17.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from nbformat) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: importlib-metadata>=5.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from circuitsvis) (8.4.0)\n",
      "Requirement already satisfied: pyyaml in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.24.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.4)\n",
      "Requirement already satisfied: filelock in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.10.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.20.0)\n",
      "Requirement already satisfied: decorator in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: sympy in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.27.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.13.0)\n",
      "Requirement already satisfied: setproctitle in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/manavdahra/workspace/mech-interp/.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipykernel setuptools transformer_lens ipywidgets plotly nbformat circuitsvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Vocabulary and Synthetic data generation:\n",
    "\n",
    "Example generation:\n",
    "\n",
    "> \\> equ78 equ87 eq7u8 eq78u eq8u7 eq87u euq78 euq87 eu7q8 eu78q eu8q7 eu87q e7qu8 e7q8u e7uq8 e7u8q e78qu e78uq e8qu7 e8q7u e8uq7 e8u7q e87qu e87uq qeu78 qeu87 qe7u8 qe78u qe8u7 qe87u que78 que87 qu7e8 qu78e qu8e7 qu87e q7eu8 q7e8u q7ue8 q7u8e q78eu q78ue q8eu7 q8e7u q8ue7 q8u7e q87eu q87ue ueq78 ueq87 ue7q8 ue78q ue8q7 ue87q uqe78 uqe87 uq7e8 uq78e uq8e7 uq87e u7eq8 u7e8q u7qe8 u7q8e u78eq u78qe u8eq7 u8e7q u8qe7 u8q7e u87eq u87qe 7equ8 7eq8u 7euq8 7eu8q 7e8qu 7e8uq 7qeu8 7qe8u 7que8 7qu8e 7q8eu 7q8ue 7ueq8 7ue8q 7uqe8 7uq8e 7u8eq 7u8qe 78equ 78euq 78qeu 78que 78ueq 78uqe 8equ7 8eq7u 8euq7 8eu7q 8e7qu 8e7uq 8qeu7 8qe7u 8que7 8qu7e 8q7eu 8q7ue 8ueq7 8ue7q 8uqe7 8uq7e 8u7eq 8u7qe 87equ 87euq 87qeu 87que 87ueq 87uqe.\n",
    "\n",
    "$n_{ctx} = 722$\n",
    "\n",
    "where $n_{ctx}$ is the context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from typing import List\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self._special_chars = \"> .\"\n",
    "        self.vocab = self._special_chars + string.ascii_letters + string.digits\n",
    "\n",
    "    def str_to_tokens(self, s: str) -> List[int]:\n",
    "        return [self.vocab.index(ch) for ch in s]\n",
    "\n",
    "    def tokens_to_str(self, tokens: List[int]) -> str:\n",
    "        return \"\".join([self.vocab[token] for token in tokens])\n",
    "    \n",
    "    def special_chars(self) -> List[str]:\n",
    "        return list(self._special_chars)\n",
    "        \n",
    "    def vocab_without_special_chars(self) -> str:\n",
    "        return self.vocab[len(self._special_chars):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data\n",
    "\n",
    "1. Use `itertools` library to generate a combination of elements from vocabulary of 62 charcaters. See [itertools](https://docs.python.org/3/library/itertools.html#itertools.combinations)\n",
    "2. Generate permutation sequence from given elements. See [itertools](https://docs.python.org/3/library/itertools.html#itertools.permutations)\n",
    "\n",
    "Properties of data:\n",
    "Sample prompt:\n",
    "> abcd abdc acbd acdb adbc adcb bacd badc bcad bcda bdac bdca cabd cadb cbad cbda cdab cdba dabc dacb dbac dbca dcab dcba.\n",
    "\n",
    "\n",
    "Total characters:\n",
    "\n",
    "$$seq\\_len(n) = \\underbrace{(n!)}_{\\text{sequence terms}} \\times \\underbrace{(n+1)}_{\\text{term length + space}} + \\underbrace{2}_{\\text{special chars}}$$\n",
    "$$seq\\_len(n) = (n+1)! + 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from itertools import permutations, combinations\n",
    "from typing import List\n",
    "import math\n",
    "import torch\n",
    "\n",
    "n_comb = 4\n",
    "tokenizer = Tokenizer()\n",
    "[beg, space, period] = tokenizer.special_chars()\n",
    "\n",
    "def sequence_len(n: int) -> int:\n",
    "    return math.factorial(n+1)+2\n",
    "\n",
    "def start_pos(seq_len: int) -> int: # reverse mapping for seq_len vs offset\n",
    "    return {8: 4, 26: 5, 122: 6, 722: 7}[seq_len]\n",
    "\n",
    "def permute(vocab: str) -> List[str]:\n",
    "    return [\"\".join(item) for item in permutations(vocab, len(vocab))]\n",
    "\n",
    "def data_from_permute(terms: List[str]) -> str:\n",
    "    output = space.join(terms)\n",
    "    \n",
    "    return f\"> {output}.\"\n",
    "\n",
    "def generate_dataset(n, combs=None):\n",
    "    if not combs:\n",
    "        elems = tokenizer.vocab_without_special_chars()\n",
    "        combs = list(combinations(elems, n))\n",
    "        shuffle(combs)\n",
    "    \n",
    "    while True:\n",
    "        for comb in combs:\n",
    "            terms = permute(\"\".join(comb))\n",
    "            yield data_from_permute(terms)\n",
    "\n",
    "n_ctx = sequence_len(n_comb)\n",
    "print(n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> zFHMU zFHUM zFMHU zFMUH zFUHM zFUMH zHFMU zHFUM zHMFU zHMUF zHUFM zHUMF zMFHU zMFUH zMHFU zMHUF zMUFH zMUHF zUFHM zUFMH zUHFM zUHMF zUMFH zUMHF FzHMU FzHUM FzMHU FzMUH FzUHM FzUMH FHzMU FHzUM FHMzU FHMUz FHUzM FHUMz FMzHU FMzUH FMHzU FMHUz FMUzH FMUHz FUzHM FUzMH FUHzM FUHMz FUMzH FUMHz HzFMU HzFUM HzMFU HzMUF HzUFM HzUMF HFzMU HFzUM HFMzU HFMUz HFUzM HFUMz HMzFU HMzUF HMFzU HMFUz HMUzF HMUFz HUzFM HUzMF HUFzM HUFMz HUMzF HUMFz MzFHU MzFUH MzHFU MzHUF MzUFH MzUHF MFzHU MFzUH MFHzU MFHUz MFUzH MFUHz MHzFU MHzUF MHFzU MHFUz MHUzF MHUFz MUzFH MUzHF MUFzH MUFHz MUHzF MUHFz UzFHM UzFMH UzHFM UzHMF UzMFH UzMHF UFzHM UFzMH UFHzM UFHMz UFMzH UFMHz UHzFM UHzMF UHFzM UHFMz UHMzF UHMFz UMzFH UMzHF UMFzH UMFHz UMHzF UMHFz.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generate_dataset(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test generator function and tokenizer\n",
    "1. Data set generator should generate expected data.\n",
    "2. Conversion of `str_to_tokens` and `tokens_to_string` should do conversions correctly.\n",
    "2. Model should work correctly for a sample data. Tensor shapes should match with expected shape.\n",
    "3. Model should incorrectly predict the permuation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_generate_correct_dataset(n: int, expected_data: str):\n",
    "    test_dataset_gen = generate_dataset(n, combs=combinations(tokenizer.vocab_without_special_chars(), n))\n",
    "    actual_data = next(test_dataset_gen)\n",
    "    assert expected_data == actual_data, f\"{expected_data} != {actual_data}\"\n",
    "\n",
    "should_generate_correct_dataset(1, \"> a.\")\n",
    "should_generate_correct_dataset(2, \"> ab ba.\")\n",
    "should_generate_correct_dataset(3, \"> abc acb bac bca cab cba.\")\n",
    "should_generate_correct_dataset(4, \"> abcd abdc acbd acdb adbc adcb bacd badc bcad bcda bdac bdca cabd cadb cbad cbda cdab cdba dabc dacb dbac dbca dcab dcba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "\n",
    "1. n_layers = 1\n",
    "2. d_model=128\n",
    "3. d_head=64\n",
    "4. n_heads=1\n",
    "5. d_vocab=65\n",
    "6. attn_only=True\n",
    "7. n_ctx=len(prompt) (722 max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import EasyTransformerConfig, EasyTransformer\n",
    "from transformer_lens.utils import get_device\n",
    "import os \n",
    "\n",
    "device = get_device()\n",
    "if device == \"mps\":\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    \n",
    "torch.set_default_device(device=device)\n",
    "\n",
    "cfg = EasyTransformerConfig(\n",
    "    n_layers=1,\n",
    "    d_model=128,\n",
    "    d_head=64,\n",
    "    n_heads=1,\n",
    "    d_mlp=None,\n",
    "    d_vocab=len(tokenizer.vocab),\n",
    "    n_ctx=n_ctx,\n",
    "    act_fn=\"solu\", # think about what activation function is best. SoLU does better than ReLU.\n",
    "    attn_only=True,\n",
    "    seed=123,\n",
    ")\n",
    "model = EasyTransformer(cfg)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_return_output_tensor_with_correct_shape(input: torch.Tensor, expected_shape: List[int]):\n",
    "    with torch.no_grad():\n",
    "        expected_shape = torch.Size(expected_shape)\n",
    "        output = model(input)\n",
    "        actual_shape = output.shape\n",
    "    \n",
    "    assert expected_shape == actual_shape, f\"{expected_shape} != {actual_shape}\"\n",
    "    \n",
    "def should_return_output_tensor_with_incorrect_prediction(input: str, expected_completion: str):\n",
    "    with torch.no_grad():\n",
    "        tokens = torch.tensor(tokenizer.str_to_tokens(input))\n",
    "        output = model(tokens)\n",
    "    \n",
    "    actual_completion = tokenizer.tokens_to_str([output[:, -1, :].argmax().item()])\n",
    "    assert expected_completion != actual_completion, f\"{expected_completion} != {actual_completion}\"\n",
    "\n",
    "test_dataset_gen = generate_dataset(2)\n",
    "test_input_seq = next(test_dataset_gen)\n",
    "test_input = tokenizer.str_to_tokens(test_input_seq)\n",
    "test_input = torch.tensor(test_input)\n",
    "\n",
    "should_return_output_tensor_with_correct_shape(test_input, [1, len(test_input), model.cfg.d_vocab])\n",
    "should_return_output_tensor_with_incorrect_prediction(\"> ab\", \"> ab ba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model prediction on sample data set.\n",
    "\n",
    "Test model outputs before it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected:> dhm dmh hdm hmd mdh mhd.\n",
      "actual:> dhm4je4hm0N4s44N5j3gu44g\n"
     ]
    }
   ],
   "source": [
    "def sample_data_test(expected: str):\n",
    "    with torch.no_grad():\n",
    "        offset = start_pos(len(expected))\n",
    "        tokens = tokenizer.str_to_tokens(expected)\n",
    "        prefix = tokens[:offset]\n",
    "        tokens = torch.tensor(tokens)\n",
    "        i = offset\n",
    "        output = []\n",
    "        while i < len(expected):\n",
    "            logits = model(tokens[:i])\n",
    "            prediction = logits[:, -1, :].argmax().item()\n",
    "            output.append(prediction)\n",
    "            i += 1\n",
    "        print(f\"expected:{expected}\")\n",
    "        print(f\"actual:{tokenizer.tokens_to_str(prefix + output)}\")\n",
    "\n",
    "sample_data_test(next(generate_dataset(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps, not surprisingly, the model outputs gibberish. It cannot do sequence prediction yet, since it is not trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set with generator functions\n",
    "1. Generator functions are handy tools for generating data lazily. Here `__getitem__` is a generator function\n",
    "2. This becomes very handy when we deal with lots of data and can't load it to memory to work from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 26) (2856955213.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    tokens = torch.tensor(tokenizer.str_to_tokens(entry), device=device\")\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 26)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from random import choice\n",
    "\n",
    "class PermutationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n:int=n_comb, \n",
    "        max_len:int=n_ctx, \n",
    "        all_combs:bool=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        generators = [generate_dataset(n)]\n",
    "        if all_combs:\n",
    "            generators = [generate_dataset(i) for i in range(2, n+1)]\n",
    "        \n",
    "        self.generators = generators\n",
    "        self.n = n\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return 1000\n",
    "        \n",
    "    def __getitem__(self, _) -> dict:\n",
    "        while True:\n",
    "            for entry in choice(self.generators):\n",
    "                tokens = torch.tensor(tokenizer.str_to_tokens(entry), device=device)\n",
    "                return {\"tokens\": tokens, \"length\": len(entry)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loader\n",
    "\n",
    "1. $test \\ratio train = 20 \\ratio 80$\n",
    "2. `collate_fn` is defined so we can train the model on variable sequence length (if needed).\n",
    "3. For now, only fixed length dataset is generated. This is to make the attention head pattern more comprehensible. \n",
    "\n",
    "> To study the pattern for variable sequence length, just feed `all_combs=True` to `PermutationDataset` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(data):\n",
    "    pad_token = tokenizer.special_chars().index(space)\n",
    "    batch_size = len(data)\n",
    "    max_len = max([d[\"length\"] for d in data])\n",
    "    \n",
    "    padded_data = torch.stack([F.pad(d[\"tokens\"], (0, max_len-d[\"length\"]), value=pad_token) for d in data])\n",
    "    attention_mask = torch.tensor([start_pos(d[\"length\"])-1 for d in data]).reshape(batch_size, 1)\n",
    "    attention_mask = (torch.arange(max_len).repeat(batch_size).reshape(batch_size, max_len) >= attention_mask).int()\n",
    "    \n",
    "    return {\"tokens\": padded_data, \"attention_mask\": attention_mask}\n",
    "\n",
    "# Switching of all_combs=True flag for now. To do an easier the analysis.\n",
    "dataset = PermutationDataset(n=n_comb, max_len=n_ctx)\n",
    "generator = torch.Generator(device=device)\n",
    "test, train = random_split(dataset=dataset, lengths=[.2, .8], generator=generator)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train, \n",
    "    batch_size=4, \n",
    "    generator=generator,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test, \n",
    "    batch_size=4, \n",
    "    generator=generator, \n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test shape and content of Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_batch_size = 4\n",
    "\n",
    "for index, batch in enumerate(train_data_loader):\n",
    "    for tokens, attn_mask in zip(batch[\"tokens\"], batch[\"attention_mask\"]):\n",
    "        print(f\"{tokenizer.tokens_to_str(tokens)}\")\n",
    "    \n",
    "    assert expected_batch_size == batch[\"tokens\"].shape[0], f\"{expected_batch_size} != {batch.shape[0]}\"\n",
    "    if index == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "1. Use synthetically generated training data from dataloader to train the model.\n",
    "2. Collect loss data at every step and generate loss vs steps graph to validate that the loss is decreasing.\n",
    "3. Loss function computes the `log_softmax` of the logits for every token predicted in the sequence.\n",
    "4. Attention mask is used to ignore tokens until first sequence term for loss calulcation. Sequence cannot be predicted until first sequence term is supplied.\n",
    "\n",
    "### Hyper parameters:\n",
    "1. learning rate    =0.0001\n",
    "2. num of epochs    =10 (default)\n",
    "3. optimizer betas  =(0.9, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from transformer_lens import EasyTransformer\n",
    "from transformer_lens.utils import lm_cross_entropy_loss\n",
    "from torch import optim\n",
    "from transformer_lens.train import train\n",
    "import os\n",
    "\n",
    "def train(\n",
    "    model: EasyTransformer,\n",
    "    num_epochs=10,\n",
    "    lr=1e-4,\n",
    "    max_grad_norm=1.0,\n",
    "    print_every=100,\n",
    "    save_dir=\"./model_weights\",\n",
    "    betas=(.9, .99),\n",
    "    save_every=None,\n",
    "    max_steps = None,\n",
    ") -> List[float]:\n",
    "    model.zero_grad()\n",
    "    model.init_weights()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, betas=betas)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in trange(1, num_epochs + 1):\n",
    "        samples = 0\n",
    "        for step, batch in enumerate(train_data_loader):\n",
    "            tokens = batch[\"tokens\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            logits = model(input=tokens)\n",
    "            \n",
    "            loss = lm_cross_entropy_loss(logits, tokens, attention_mask)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            samples += tokens.shape[0]\n",
    "\n",
    "            if save_every is not None and step % save_every == 0 and save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{save_dir}/model_{step}.pt\")\n",
    "                \n",
    "            if print_every is not None and step % print_every == 0:\n",
    "                print(f\"Epoch {epoch} Samples {samples} Step {step} Loss {loss.item()}\")\n",
    "\n",
    "            if max_steps is not None and step >= max_steps:\n",
    "                break\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses = train(model=model, num_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss vs steps graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(losses, labels={\"index\": \"steps\", \"value\": \"loss\", \"title\": \"Loss vs steps\", \"variable\": \"loss\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"loss_vs_steps_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "This is promising!\n",
    "\n",
    "Clearly the loss decreases rapidly within first few epochs. \n",
    "\n",
    "Hence, model must have learned to do permutation sequence prediction given what we have observed.\n",
    "\n",
    "We can validate this observation by testing on a test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model accuracy\n",
    "1. Accuracy function is defined similar to a loss function before, except this time we count how many tokens in the predicted sequence match the expected sequence tokens.\n",
    "2. Accuracy computation is done ignoring the first `2+n_comb` positions. This is needed to make sure model is not penalised unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import lm_accuracy\n",
    "\n",
    "def validate_model(model: EasyTransformer):\n",
    "    with torch.no_grad():\n",
    "        accuracies = []\n",
    "        for _, batch in enumerate(test_data_loader):\n",
    "            logits = model(input=batch[\"tokens\"])\n",
    "            accuracy = lm_accuracy(logits, batch[\"tokens\"])\n",
    "            accuracies.append(accuracy)\n",
    "    \n",
    "    return sum(accuracies)/len(accuracies)\n",
    "\n",
    "accuracy = validate_model(model)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is very close to `~1.0`!\n",
    "\n",
    "We may conclude that the model has indeed learned how to generate a permutation sequence. Where the length of the sequence is between the range of [2, 5].\n",
    "\n",
    "Let's check if the output being generated by model is accurate enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(batch: torch.Tensor):\n",
    "    attn_mask = batch[\"attention_mask\"]\n",
    "    tokens = batch[\"tokens\"]\n",
    "    logits = model(input=tokens)\n",
    "    \n",
    "    predictions = logits.argmax(-1)\n",
    "    for i in range(len(predictions)):\n",
    "        print(tokenizer.tokens_to_str(tokens[i]))\n",
    "        zeros = attn_mask.shape[1] - torch.count_nonzero(attn_mask[i]).item()\n",
    "        prefix = torch.cat((tokens[i,:zeros+1], predictions[i,zeros:]), dim=-1)\n",
    "        print(tokenizer.tokens_to_str(prefix))\n",
    "        \n",
    "    print(f\"Accuracy: {lm_accuracy(logits, batch[\"tokens\"])}\")\n",
    "    print(f\"Loss: {lm_cross_entropy_loss(logits, batch[\"tokens\"], attn_mask)}\")\n",
    "\n",
    "batch = next(iter(train_data_loader))\n",
    "check(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty accurate! We can now try to understand what model has learned. Maybe we can get some insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Since we have only 1 attention and 1 layer, this means all the permutation generation logic is encoded in the single attention head.\n",
    "\n",
    "Let's see if we can analyze what that attention head has actually learned after training.\n",
    "\n",
    "We can start by plotting a heat map of attention scores between every pair of positions $(i,j)$ \n",
    "\n",
    "where, $i = source$ and $j = destination$.\n",
    "\n",
    "> Note: This model only has 1-Layer with only 1 attention head. This means the OV circuit behaviour will not come into play. \n",
    "\n",
    "From the analysis in this paper: [Detecting copy behaviour](https://transformer-circuits.pub/2021/framework/index.html#:~:text=DETECTING%20COPYING%20BEHAVIOR)\n",
    "\n",
    "A single attention head is capable of doing copy behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import to_numpy\n",
    "\n",
    "def imshow(tensor, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    tensor = to_numpy(tensor)\n",
    "    plot_kwargs = {\n",
    "        \"color_continuous_scale\":\"RdBu\", \n",
    "        \"color_continuous_midpoint\":0.0, \n",
    "        \"labels\":{\"x\": xaxis, \"y\": yaxis}, \n",
    "        \"width\": 1024, \n",
    "        \"height\": 1024,\n",
    "    }\n",
    "    plot_kwargs.update(kwargs)\n",
    "    fig = px.imshow(tensor, **plot_kwargs)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = data_from_permute(permute(\"iFZ9\"))\n",
    "test_input_tokens = tokenizer.str_to_tokens(test_input)\n",
    "logits, cache = model.run_with_cache(torch.tensor(test_input_tokens))\n",
    "\n",
    "for k in cache:\n",
    "    print(f\"{k} {cache[k].shape}\")\n",
    "\n",
    "attn_pattern = cache[\"pattern\", 0]\n",
    "\n",
    "limit = sequence_len(n_comb)\n",
    "token_labels = [f\"{c},{i}\" for i, c in enumerate(test_input[:limit])]\n",
    "opts = {\n",
    "    \"x\": token_labels,\n",
    "    \"y\": token_labels,\n",
    "}\n",
    "\n",
    "print(test_input)\n",
    "imshow(attn_pattern[-1,-1][:limit,:limit], xaxis=\"dst_token\", yaxis=\"src_token\", **opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. What does the figure showing attention pattern mean ?\n",
    "2. Why top left corner of the attention head pattern shows higher scores (darker blue colors) ? \n",
    "3. Why do we have regular spacing between activations\n",
    "4. What happens if we feed duplicate characters, will the model predict sequence correctly ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and conclusion\n",
    "\n",
    "<img src=\"./attn_head_pattern_annotated.png\" height=800>\n",
    "\n",
    "1. Zooming in, it is clear that the permutation sequence is being generated by attending to tokens having periodic gaps in the QK circuit.\n",
    "    \n",
    "    <img src=\"./zoomed_in_pattern.png\" height=800>\n",
    "    \n",
    "    Information of token `i` at position `2` is attending to position `6`. This makes up the Key.\n",
    "    When, query is done on this position, asking what is the token to attend to, then $softmax(Q \\cdot K)$ gives a high activation value for position 6.\n",
    "    Token at position `7` is therefore takes up `i,2` into consideration.\n",
    "    \n",
    "2. Activations of `src_token = \" \"` takes up regular intervals of `5` in vertical direction.\n",
    "    \n",
    "    This makes sense, as the term length characters and activation of `\" \"` will happen regularly.\n",
    "3. More interesting question is what is the pattern of activations of non-space characters ? \n",
    "    \n",
    "    Other characters don't follow a strict spacing pattern of activation, as the characters once in a while do a swap in subsequent terms. \n",
    "    Due to this, the spacing interval is not regular and grows and shrinks regularly.\n",
    "    I have not been able to work out the mathematics for this yet. But I suspect it is something that other researchers might have already figured out.\n",
    "\n",
    "4. There are no significant activations/copying of `src_tokens` at later positions\n",
    "\n",
    "    One explanation could be that since the characters repeat quite a lot in the sequence, it is sufficient for the model to just pay attention to first few character positions to predict the whole sequence. Similar to the explanation of 2 activations of space at position number `20`.\n",
    "    We can test this assumption by shrinking the context length of the model under `122` characters, the model should then activate more positions in the lower diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OV Circuit analysis\n",
    "\n",
    "Next let's analyze the OV-circuit which is responsible for telling us what token value is picked up when attention is taken into account by the attention head.\n",
    "Given, what we analysed so far, the QK-circuit in attention head is paying attention to a token position based on a periodic gapping pattern. \n",
    "\n",
    "Token `i` at position `2` is attended to token `\" \"` at position `6` so OV-circuit should just copy that token to new position `7`.\n",
    "\n",
    "Now, this implies that if we plot a 2-D img of \n",
    "$W_E \\cdot OV \\cdot W_U$ for 1st layer and 1st head, \n",
    "\n",
    "then, we should see a diagonal line having high activations.\n",
    "\n",
    "Diagonal line will be the line where $token_{src} = token_{dst}$, this should represent copying of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_OV_circuit = model.embed.W_E @ model.OV @ model.unembed.W_U\n",
    "vocab_tokens = [c for c in list(tokenizer.vocab) if c != '.']\n",
    "opts = {\n",
    "    \"x\": vocab_tokens,\n",
    "    \"y\": vocab_tokens,\n",
    "    \"labels\": {\"x\": \"output\", \"y\": \"value\"},\n",
    "}\n",
    "imshow(to_numpy(full_OV_circuit.BA[-1,-1]), **opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ov_circuit.png\" height=800>\n",
    "\n",
    "Above is a strong evidence to suggest that indeed copying is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation/Demo of attention on token\n",
    "\n",
    "`circuitvis` is neat library that helps with such visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.tokens import colored_tokens_multi\n",
    "\n",
    "print(cache[\"pattern\", 0][-1,-1].shape)\n",
    "colored_tokens_multi(tokens=list(test_input), values=cache[\"pattern\", 0][-1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow up. can the model do sequence generation for duplicate tokens ?\n",
    "\n",
    "Yes. Even though the data set did not contain repeating characters, the model can still output sequence for repeating characters. \n",
    "This shows that model has learned sequence generation using positions and not the value of tokens itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character g is duplicate\n",
    "dup_data = \"> ghg4 gh4g ggh4 gg4h g4hg g4gh hgg4 hg4g hgg4 hg4g h4gg h4gg ggh4 gg4h ghg4 gh4g g4gh g4hg 4ghg 4ggh 4hgg 4hgg 4ggh 4ghg.\"\n",
    "sample_data_test(dup_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The Attention pattern heat map seems to point towards the core of how this model is predicting the next token of the sequence. \n",
    "More rigor is required to prove that QK offset is doing what has been hypothesized but given evidence seems fairly convincing.\n",
    "\n",
    "Working on this problem was a great learning opportunity and I hope to contribute to the field of Mechanistic interpretability in future. \n",
    "Thanks again Neel! Your contributions have been amazing with `transformer_lens` library. I hope we get to collaborate in future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
